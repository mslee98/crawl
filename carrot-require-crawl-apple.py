"""
ë‹¹ê·¼ë§ˆì¼“ í¬ë¡¤ë§ - ì• í”Œ ê³ ì • ë²„ì „
- ê²€ìƒ‰ í‚¤ì›Œë“œ: ì• í”Œ (ê³ ì •)
- íŒë§¤ì™„ë£Œ(ê±°ë˜ì™„ë£Œ)ë§Œ ìˆ˜ì§‘
- ê¸ˆì•¡ 35,000ì› ì´ìƒë§Œ ì¶”ì¶œ
"""
import argparse
import asyncio
import csv
import re
import time
from datetime import datetime
from pathlib import Path
from urllib.parse import quote
from playwright.async_api import async_playwright

RESULTS_DIR = Path("results")

# =========================
# ğŸ”¥ ì• í”Œ ê³ ì • ë²„ì „ ì „ì—­ ì„¤ì •
# =========================

HEADLESS = False          # Trueë©´ ë¸Œë¼ìš°ì € ì•ˆë³´ì„
SLOW_MO = 0               # ë™ì‘ ëŠë¦¬ê²Œ ë³´ê³  ì‹¶ìœ¼ë©´ 100~300
TARGET_COUNT = 1000

# ì• í”Œ ê³ ì • + íŒë§¤ì™„ë£Œ + 35,000ì› ì´ìƒ
DEFAULT_KEYWORD = "ì• í”Œ"
DEFAULT_MIN_PRICE = 35000
SOLD_STATUSES = ("ê±°ë˜ì™„ë£Œ", "íŒë§¤ì™„ë£Œ")   # ì´ ìƒíƒœë§Œ ìˆ˜ì§‘

ITEM_SELECTOR = "a[data-gtm='search_article']"
MORE_BUTTON_SELECTOR = "div[data-gtm='search_show_more_articles'] button"

# ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘
DETAIL_PAGE_DELAY_MS = 800   # ë°°ì¹˜/ìš”ì²­ ê°„ ëŒ€ê¸° (ms)
DETAIL_PAGE_DELAY_MS_ON_FAIL = 200   # ìƒì„¸ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ëŒ€ê¸° (ms)
DETAIL_PAGE_TIMEOUT_MS = 15000
DETAIL_PAGE_CONCURRENCY = 4   # ë™ì‹œ ìƒì„¸ ìˆ˜ì§‘ ìˆ˜ (2~5 ê¶Œì¥)
DETAIL_PAGE_WAIT_SELECTOR = "#main-content article"   # ìƒì„¸ ë¡œë“œ ì™„ë£Œ íŒë‹¨ìš©
DETAIL_PAGE_WAIT_TIMEOUT_MS = 5000
DETAIL_PAGE_FALLBACK_MS = 200   # selector ëŒ€ê¸° ì‹¤íŒ¨ ì‹œ ì¶”ê°€ ëŒ€ê¸°

# ë”ë³´ê¸° í´ë¦­ í›„ ëŒ€ê¸° (ì¡°ê±´ë¶€)
MORE_BUTTON_POLL_INTERVAL_MS = 200   # ì¹´ë“œ ìˆ˜ ì¦ê°€ í™•ì¸ ê°„ê²©
MORE_BUTTON_POLL_MAX_MS = 5000   # ìµœëŒ€ ëŒ€ê¸°

# ë¦¬ìŠ¤íŠ¸ ì²« ë¡œë“œ
LIST_PAGE_WAIT_SELECTOR_TIMEOUT_MS = 10000

# ë‹¹ê·¼ ì¹´í…Œê³ ë¦¬ (í•„í„°ìš©)
ALL_CATEGORIES = [
    "ë””ì§€í„¸ê¸°ê¸°", "ìƒí™œê°€ì „", "ê°€êµ¬/ì¸í…Œë¦¬ì–´", "ìƒí™œ/ì£¼ë°©", "ìœ ì•„ë™", "ìœ ì•„ë„ì„œ",
    "ì—¬ì„±ì˜ë¥˜", "ì—¬ì„±ì¡í™”", "ë‚¨ì„±íŒ¨ì…˜/ì¡í™”", "ë·°í‹°/ë¯¸ìš©", "ìŠ¤í¬ì¸ /ë ˆì €", "ì·¨ë¯¸/ê²Œì„/ìŒë°˜",
    "ë„ì„œ", "í‹°ì¼“/êµí™˜ê¶Œ", "eì¿ í°", "ê°€ê³µì‹í’ˆ", "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ", "ë°˜ë ¤ë™ë¬¼ìš©í’ˆ", "ì‹ë¬¼",
    "ê¸°íƒ€ ì¤‘ê³ ë¬¼í’ˆ", "ì‚½ë‹ˆë‹¤",
]
# ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ (ë¹„ì–´ ìˆìœ¼ë©´ í•„í„° ì—†ìŒ = ì „ì²´ ìˆ˜ì§‘)
ALLOWED_CATEGORIES = ["ë””ì§€í„¸ê¸°ê¸°", "ë‚¨ì„±íŒ¨ì…˜/ì¡í™”", "í‹°ì¼“/êµí™˜ê¶Œ", "eì¿ í°"]

# =========================


def _parse_price(price_str: str) -> int | None:
    """ê°€ê²© ë¬¸ìì—´ì„ ì› ë‹¨ìœ„ ì •ìˆ˜ë¡œ ë³€í™˜. íŒŒì‹± ì‹¤íŒ¨ ì‹œ None."""
    if not price_str or not isinstance(price_str, str):
        return None
    s = price_str.strip().replace(",", "").replace("ì›", "").strip()
    numbers = re.findall(r"\d+", s)
    if not numbers:
        return None
    return int(numbers[0])


def _build_search_url(
    keyword: str | None = None,
    region: str | None = None,
    min_price: int | None = None,
    max_price: int | None = None,
) -> str:
    """ê²€ìƒ‰ í‚¤ì›Œë“œÂ·ì§€ì—­Â·ê°€ê²©ìœ¼ë¡œ ë‹¹ê·¼ ê²€ìƒ‰ URL ìƒì„±. ê°€ê²©ì€ price=ìµœì†Œ__ìµœëŒ€ í˜•ì‹."""
    base = "https://www.daangn.com/kr/buy-sell/"
    if keyword and keyword.strip():
        url = f"{base}?search={quote(keyword.strip())}"
    else:
        url = base
    if min_price is not None or max_price is not None:
        price_val = f"{min_price or ''}__{max_price or ''}"
        url += "&" if "?" in url else "?"
        url += f"price={price_val}"
    return url


def _extract_detail_js() -> str:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ íƒ€ì´í‹€Â·ì£¼ì†ŒÂ·ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œí•˜ëŠ” JS."""
    return """
    () => {
        const out = { title: "", location: "", category: "" };
        const titleEl = document.querySelector('#main-content article div._4y5lbr4 h1') || document.querySelector('#main-content article h1') || document.querySelector('article h1');
        if (titleEl) out.title = titleEl.innerText.trim();
        const catH2 = document.querySelector('#main-content article section:nth-of-type(2) div h2._4y5lbr9') || document.querySelector('#main-content article section:nth-of-type(2) div h2');
        if (catH2) {
            const catLink = catH2.querySelector('a[href*="category_id"]') || catH2.querySelector('a');
            if (catLink) out.category = catLink.innerText.trim();
        }
        const profileAnchor = document.querySelector('a[aria-label*="í”„ë¡œí•„"]');
        if (profileAnchor) {
            const container = profileAnchor.closest('div');
            if (container) {
                const locLink = container.querySelector('a[href*="in="]');
                if (locLink) out.location = locLink.innerText.trim();
            }
        }
        return out;
    }
    """


async def _fetch_detail(page, url: str) -> dict:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ íƒ€ì´í‹€Â·ì£¼ì†ŒÂ·ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œí•´ ë°˜í™˜."""
    fail_result = {"title": "", "location": "", "category": ""}
    try:
        await page.goto(url, wait_until="domcontentloaded", timeout=DETAIL_PAGE_TIMEOUT_MS)
        try:
            await page.wait_for_selector(DETAIL_PAGE_WAIT_SELECTOR, timeout=DETAIL_PAGE_WAIT_TIMEOUT_MS)
        except Exception:
            await page.wait_for_timeout(DETAIL_PAGE_FALLBACK_MS)
        data = await page.evaluate(_extract_detail_js())
        return data
    except Exception:
        return fail_result


def _parse_args():
    parser = argparse.ArgumentParser(
        description="ë‹¹ê·¼ë§ˆì¼“ í¬ë¡¤ë§ - ì• í”Œ ê³ ì • (íŒë§¤ì™„ë£Œ, 35,000ì› ì´ìƒ)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
í•„í„° ì˜µì…˜ ì •ë¦¬
---------------
  ìƒíƒœ í•„í„° (ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì‚¬ìš©)
    --sold-only      ê±°ë˜ì™„ë£Œ/íŒë§¤ì™„ë£Œë§Œ ìˆ˜ì§‘ (ê¸°ë³¸ê°’). ê²€ìƒ‰ ê²°ê³¼ì— ì™„ë£Œ ê¸€ì´ ì—†ìœ¼ë©´ 0ê±´ì´ ë¨.
    --no-sold-only   ìƒíƒœ ë¬´ì‹œ, ì „ë¶€ ìˆ˜ì§‘ (ê°€ê²©/ì¹´í…Œê³ ë¦¬ë§Œ ì ìš©). 283ê±´ ìœ ì§€í•˜ë ¤ë©´ ì´ ì˜µì…˜ ì‚¬ìš©.

  ê°€ê²© í•„í„°
    --min-price N    Nì› ì´ìƒë§Œ (ê¸°ë³¸: 35000)
    --max-price N    Nì› ì´í•˜ë§Œ (ê¸°ë³¸: ì—†ìŒ)

  ì¹´í…Œê³ ë¦¬ í•„í„°
    -c ë””ì§€í„¸ê¸°ê¸°,í‹°ì¼“/êµí™˜ê¶Œ   í•´ë‹¹ ì¹´í…Œê³ ë¦¬ë§Œ
    --no-filter                 ì¹´í…Œê³ ë¦¬ í•„í„° ì—†ì´ ì „ì²´

  ê²€ìƒ‰
    -k, --keyword    ê²€ìƒ‰ì–´ (ê¸°ë³¸: ì• í”Œ)

ì˜ˆì‹œ
----
  # 5ë§Œì› ì´ìƒë§Œ, ìƒíƒœ ìƒê´€ì—†ì´ ì „ë¶€ ìˆ˜ì§‘ (283ê±´ ìœ ì§€)
  python carrot-require-crawl-apple.py --min-price 50000 --no-sold-only

  # íŒë§¤ì™„ë£Œë§Œ + 5ë§Œì› ì´ìƒ (ì™„ë£Œ ê¸€ì´ ìˆì–´ì•¼ í•¨)
  python carrot-require-crawl-apple.py --min-price 50000

  # ì¹´í…Œê³ ë¦¬ ì—†ì´ ì „ë¶€, 3ë§Œ5ì²œì› ì´ìƒ
  python carrot-require-crawl-apple.py --no-filter --no-sold-only
""",
    )
    parser.add_argument("--keyword", "-k", default=DEFAULT_KEYWORD, help=f"ê²€ìƒ‰ í‚¤ì›Œë“œ (ê¸°ë³¸: {DEFAULT_KEYWORD})")
    parser.add_argument(
        "--categories", "-c",
        default=None,
        help="ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œ êµ¬ë¶„). ë¹„ìš°ë©´ ìŠ¤í¬ë¦½íŠ¸ ê¸°ë³¸ê°’ ì‚¬ìš©, --no-filter ì´ë©´ ì „ì²´ ìˆ˜ì§‘",
    )
    parser.add_argument("--no-filter", action="store_true", help="ì¹´í…Œê³ ë¦¬ í•„í„° ì—†ì´ ì „ì²´ ìˆ˜ì§‘")
    parser.add_argument("--min-price", type=int, default=DEFAULT_MIN_PRICE, metavar="N", help=f"ê°€ê²© ìµœì†Œê°’ ì› (ê¸°ë³¸: {DEFAULT_MIN_PRICE})")
    parser.add_argument("--max-price", type=int, default=None, metavar="N", help="ê°€ê²© ìµœëŒ€ê°’ (ì›)")
    parser.add_argument("--sold-only", action="store_true", default=True, dest="sold_only", help="ê±°ë˜ì™„ë£Œ/íŒë§¤ì™„ë£Œë§Œ ìˆ˜ì§‘ (ê¸°ë³¸)")
    parser.add_argument("--no-sold-only", action="store_false", dest="sold_only", help="ìƒíƒœ ë¬´ì‹œ, ê°€ê²©/ì¹´í…Œê³ ë¦¬ë§Œ ì ìš© (0ê±´ ë°©ì§€)")
    return parser.parse_args()


async def main(
    keyword: str | None = None,
    allowed_categories: list[str] | None = None,
    no_filter: bool = False,
    min_price: int | None = None,
    max_price: int | None = None,
    sold_only: bool = True,
):
    keyword = keyword or DEFAULT_KEYWORD
    min_price = min_price if min_price is not None else DEFAULT_MIN_PRICE

    if no_filter:
        allowed_set = None
    elif allowed_categories is None:
        allowed_set = set(ALLOWED_CATEGORIES)
    else:
        allowed_set = set(allowed_categories) if allowed_categories else None

    search_url = _build_search_url(keyword, min_price=min_price, max_price=max_price)
    print("[ì• í”Œ ê³ ì • ë²„ì „]", "íŒë§¤ì™„ë£Œë§Œ + " if sold_only else "ì „ì²´ ìƒíƒœ + ", f"{min_price}ì› ì´ìƒ")
    print("ê²€ìƒ‰ URL:", search_url)
    print("í‚¤ì›Œë“œ:", keyword)
    print("ê°€ê²© ì¡°ê±´:", f"{min_price}ì› ì´ìƒ", f"~ {max_price}ì›" if max_price else "")
    if allowed_set:
        print("ì¹´í…Œê³ ë¦¬ í•„í„°:", ", ".join(sorted(allowed_set)))
    else:
        print("ì¹´í…Œê³ ë¦¬ í•„í„°: ì—†ìŒ (ì „ì²´ ìˆ˜ì§‘)")

    start_time = time.perf_counter()
    print("í¬ë¡¤ë§ ì‹œì‘")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO
        )
        page = await browser.new_page()

        await page.goto(search_url, wait_until="domcontentloaded")
        await page.wait_for_selector(ITEM_SELECTOR, timeout=LIST_PAGE_WAIT_SELECTOR_TIMEOUT_MS)

        print("í˜ì´ì§€ íƒ€ì´í‹€:", await page.title())

        prev_count = 0

        # =========================
        # ğŸ”¥ ë”ë³´ê¸° ë°˜ë³µ
        # =========================
        while True:
            cards = page.locator(ITEM_SELECTOR)
            count = await cards.count()
            print("í˜„ì¬ ê°œìˆ˜:", count)

            if count >= TARGET_COUNT:
                print("ëª©í‘œ ê°œìˆ˜ ë„ë‹¬")
                break

            if count == prev_count:
                print("ë” ì´ìƒ ì¦ê°€í•˜ì§€ ì•ŠìŒ")
                break

            prev_count = count

            more_btn = page.locator(MORE_BUTTON_SELECTOR)

            if await more_btn.count() == 0:
                print("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ â†’ ì¢…ë£Œ")
                break

            if not await more_btn.is_enabled():
                print("ë”ë³´ê¸° ë²„íŠ¼ ë¹„í™œì„±í™” â†’ ì¢…ë£Œ")
                break

            try:
                await more_btn.click()
                deadline = time.monotonic() + MORE_BUTTON_POLL_MAX_MS / 1000
                while True:
                    await asyncio.sleep(MORE_BUTTON_POLL_INTERVAL_MS / 1000)
                    new_count = await cards.count()
                    if new_count > prev_count:
                        break
                    if time.monotonic() >= deadline:
                        break
            except Exception as e:
                print("ë”ë³´ê¸° í´ë¦­ ì‹¤íŒ¨:", e)
                break

        # =========================
        # ë°ì´í„° ì¶”ì¶œ
        # =========================
        
        items = await page.evaluate("""
        () => {
            const cards = document.querySelectorAll("a[data-gtm='search_article']");
            const results = [];

            cards.forEach(card => {

                const href = card.getAttribute("href") || "";
                const fullUrl = href ? "https://www.daangn.com" + href : "";

                const wrapper = card.querySelector(":scope > div");
                if (!wrapper) return;

                const children = wrapper.querySelectorAll(":scope > div");
                if (children.length < 2) return;

                const thumbnailArea = children[0];
                const textContainer = children[1];

                // íŒë§¤ìƒíƒœ (ê±°ë˜ì™„ë£Œ / íŒë§¤ì™„ë£Œ / ì˜ˆì•½ì¤‘ / íŒë§¤ì¤‘)
                let status = "íŒë§¤ì¤‘";
                const statusSpan = thumbnailArea.querySelector("span");
                if (statusSpan) {
                    const text = statusSpan.innerText.trim();
                    if (text === "ì˜ˆì•½ì¤‘" || text === "ê±°ë˜ì™„ë£Œ" || text === "íŒë§¤ì™„ë£Œ") {
                        status = text;
                    }
                }

                const textDivs = textContainer.querySelectorAll(":scope > div");
                if (textDivs.length < 2) return;

                const infoDiv = textDivs[0];
                const metaDiv = textDivs[1];

                const spans = infoDiv.querySelectorAll("span");

                const title = spans[0]?.innerText?.trim() || "";
                const price = spans[1]?.innerText?.trim() || "";

                const location = metaDiv.querySelector("span span")?.innerText?.trim() || "";
                const time = metaDiv.querySelector("time")?.innerText?.trim() || "";
                const categoryEl = card.querySelector('a[href*="category_id"]');
                const category = categoryEl ? categoryEl.innerText.trim() : "";

                if (!title) return;

                results.push({
                    title,
                    price,
                    location,
                    time,
                    status,
                    url: fullUrl,
                    category
                });
            });

            return results;
        }
        """)

        # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        seen_urls = set()
        items_deduped = []
        for i in items:
            u = i.get("url") or ""
            if u and u not in seen_urls:
                seen_urls.add(u)
                items_deduped.append(i)
        if len(items_deduped) < len(items):
            print(f"URL ì¤‘ë³µ ì œê±°: {len(items)} â†’ {len(items_deduped)}ê±´")
        items = items_deduped

        print(f"ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ê°œìˆ˜: {len(items)}")

        # ğŸ”¥ ìƒíƒœ í•„í„°(ì„ íƒ) + ê°€ê²© í•„í„°
        min_price_val = min_price
        max_price_val = max_price  # Noneì´ë©´ ìƒí•œ ì—†ìŒ
        items_filtered = []
        for i in items:
            if sold_only:
                status = (i.get("status") or "").strip()
                if status not in SOLD_STATUSES:
                    continue
            price_num = _parse_price(i.get("price") or "")
            if price_num is None or price_num < min_price_val:
                continue
            if max_price_val is not None and price_num > max_price_val:
                continue
            items_filtered.append(i)
        filter_desc = f"{min_price_val}ì› ì´ìƒ"
        if max_price_val is not None:
            filter_desc += f" ~ {max_price_val}ì› ì´í•˜"
        if sold_only:
            filter_desc = "íŒë§¤ì™„ë£Œ + " + filter_desc
        print(f"í•„í„° ({filter_desc}): {len(items)} â†’ {len(items_filtered)}ê±´")
        items = items_filtered

        if not items:
            print("ì¡°ê±´ì— ë§ëŠ” ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
            await browser.close()
            return

        # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¹´í…Œê³ ë¦¬ ì•Œ ìˆ˜ ìˆìœ¼ë©´ ë¯¸ë¦¬ í•„í„° â†’ ìƒì„¸ ë°©ë¬¸ íšŸìˆ˜ ê°ì†Œ
        if allowed_set:
            known_allowed = [i for i in items if i.get("category") in allowed_set]
            unknown = [i for i in items if not (i.get("category") or "").strip()]
            items_to_detail = known_allowed + unknown
            skipped = len(items) - len(items_to_detail)
            if skipped > 0:
                print(f"ì¹´í…Œê³ ë¦¬ í•„í„°ë¡œ ìƒì„¸ ìƒëµ: {skipped}ê±´ (ìƒì„¸ ìˆ˜ì§‘ ëŒ€ìƒ: {len(items_to_detail)}ê±´)")
        else:
            items_to_detail = items

        # =========================
        # ğŸ”¥ ìƒì„¸ í˜ì´ì§€ ì¶”ê°€ ìˆ˜ì§‘ (ë³‘ë ¬)
        # =========================
        total = len(items_to_detail)
        concurrency = min(DETAIL_PAGE_CONCURRENCY, total) if total else 0
        detail_pages = []

        if total > 0 and concurrency > 0:
            detail_pages = [await browser.new_page() for _ in range(concurrency)]
            print(f"ìƒì„¸ ìˆ˜ì§‘ ë³‘ë ¬ ìˆ˜: {concurrency}")

        for chunk_start in range(0, total, concurrency if concurrency else 1):
            chunk = items_to_detail[chunk_start : chunk_start + concurrency]
            for i, item in enumerate(chunk):
                print(f"ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ {chunk_start + i + 1}/{total} - {item.get('title', '')[:30]}...")
            tasks = [_fetch_detail(detail_pages[j], chunk[j]["url"]) for j in range(len(chunk))]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for j, item in enumerate(chunk):
                r = results[j]
                if isinstance(r, Exception):
                    extra = {"title": "", "location": "", "category": ""}
                else:
                    extra = r
                if extra.get("title"):
                    item["title"] = extra["title"]
                if extra.get("location"):
                    item["location"] = extra["location"]
                if extra.get("category"):
                    item["category"] = extra["category"]
            any_fail = any(isinstance(r, Exception) for r in results)
            delay_ms = DETAIL_PAGE_DELAY_MS_ON_FAIL if any_fail else DETAIL_PAGE_DELAY_MS
            if chunk_start + len(chunk) < total:
                await asyncio.sleep(delay_ms / 1000)

        for p in detail_pages:
            await p.close()

        # ìƒì„¸ì—ì„œ í™•ì¸í•œ ì¹´í…Œê³ ë¦¬ë¡œ í•œ ë²ˆ ë” í•„í„°
        if allowed_set:
            items_to_write = [i for i in items_to_detail if i.get("category") in allowed_set]
            print(f"ì¹´í…Œê³ ë¦¬ í•„í„° ê²°ê³¼: {len(items_to_write)}ê±´ ì €ì¥")
        else:
            items_to_write = items_to_detail

        # =========================
        # ğŸ”¥ CSV ì €ì¥ (results/ë…„-ì›”-ì¼-ì‹œ-ë¶„-ì´ˆ.csv)
        # =========================
        RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
        out_path = RESULTS_DIR / f"apple-sold-{timestamp}.csv"
        fieldnames = ["title", "price", "location", "time", "status", "category"]
        rows = [{k: item.get(k, "") for k in fieldnames} for item in items_to_write]
        with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(rows)

        print(f"{out_path} ì €ì¥ ì™„ë£Œ (íŒë§¤ì™„ë£Œ, {min_price}ì› ì´ìƒ)")

        elapsed = time.perf_counter() - start_time
        m, s = divmod(int(elapsed), 60)
        if m > 0:
            print(f"ì´ í¬ë¡¤ë§ ì‹œê°„: {m}ë¶„ {s}ì´ˆ ({elapsed:.1f}ì´ˆ)")
        else:
            print(f"ì´ í¬ë¡¤ë§ ì‹œê°„: {elapsed:.1f}ì´ˆ")

        await browser.close()


if __name__ == "__main__":
    args = _parse_args()
    if args.categories is not None:
        allowed = [c.strip() for c in args.categories.split(",") if c.strip()]
    else:
        allowed = None
    asyncio.run(main(
        keyword=args.keyword,
        allowed_categories=allowed,
        no_filter=args.no_filter,
        min_price=args.min_price,
        max_price=args.max_price,
        sold_only=args.sold_only,
    ))
