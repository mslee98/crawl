import argparse
import asyncio
import csv
import time
from datetime import datetime
from pathlib import Path
from urllib.parse import quote
from playwright.async_api import async_playwright

RESULTS_DIR = Path("results")

# =========================
# ğŸ”¥ ì „ì—­ ì„¤ì • (ì¸ì ì—†ì„ ë•Œ ê¸°ë³¸ê°’)
# =========================

HEADLESS = True          # Trueë©´ ë¸Œë¼ìš°ì € ì•ˆë³´ì„
SLOW_MO = 0               # ë™ì‘ ëŠë¦¬ê²Œ ë³´ê³  ì‹¶ìœ¼ë©´ 100~300
TARGET_COUNT = 1000

ITEM_SELECTOR = "a[data-gtm='search_article']"
MORE_BUTTON_SELECTOR = "div[data-gtm='search_show_more_articles'] button"

# ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘
DETAIL_PAGE_DELAY_MS = 800   # ë°°ì¹˜/ìš”ì²­ ê°„ ëŒ€ê¸° (ms)
DETAIL_PAGE_DELAY_MS_ON_FAIL = 200   # ìƒì„¸ ì‹¤íŒ¨ ì‹œ ë‹¤ìŒ ëŒ€ê¸° (ms)
DETAIL_PAGE_TIMEOUT_MS = 15000
DETAIL_PAGE_CONCURRENCY = 4   # ë™ì‹œ ìƒì„¸ ìˆ˜ì§‘ ìˆ˜ (2~5 ê¶Œì¥)
DETAIL_PAGE_WAIT_SELECTOR = "#main-content article"   # ìƒì„¸ ë¡œë“œ ì™„ë£Œ íŒë‹¨ìš©
DETAIL_PAGE_WAIT_TIMEOUT_MS = 5000
DETAIL_PAGE_FALLBACK_MS = 200   # selector ëŒ€ê¸° ì‹¤íŒ¨ ì‹œ ì¶”ê°€ ëŒ€ê¸°

# ë”ë³´ê¸° í´ë¦­ í›„ ëŒ€ê¸° (ì¡°ê±´ë¶€)
MORE_BUTTON_POLL_INTERVAL_MS = 200   # ì¹´ë“œ ìˆ˜ ì¦ê°€ í™•ì¸ ê°„ê²©
MORE_BUTTON_POLL_MAX_MS = 5000   # ìµœëŒ€ ëŒ€ê¸°

# ë¦¬ìŠ¤íŠ¸ ì²« ë¡œë“œ
LIST_PAGE_WAIT_SELECTOR_TIMEOUT_MS = 10000

# ë‹¹ê·¼ ì¹´í…Œê³ ë¦¬ (í•„í„°ìš©)
ALL_CATEGORIES = [
    "ë””ì§€í„¸ê¸°ê¸°", "ìƒí™œê°€ì „", "ê°€êµ¬/ì¸í…Œë¦¬ì–´", "ìƒí™œ/ì£¼ë°©", "ìœ ì•„ë™", "ìœ ì•„ë„ì„œ",
    "ì—¬ì„±ì˜ë¥˜", "ì—¬ì„±ì¡í™”", "ë‚¨ì„±íŒ¨ì…˜/ì¡í™”", "ë·°í‹°/ë¯¸ìš©", "ìŠ¤í¬ì¸ /ë ˆì €", "ì·¨ë¯¸/ê²Œì„/ìŒë°˜",
    "ë„ì„œ", "í‹°ì¼“/êµí™˜ê¶Œ", "eì¿ í°", "ê°€ê³µì‹í’ˆ", "ê±´ê°•ê¸°ëŠ¥ì‹í’ˆ", "ë°˜ë ¤ë™ë¬¼ìš©í’ˆ", "ì‹ë¬¼",
    "ê¸°íƒ€ ì¤‘ê³ ë¬¼í’ˆ", "ì‚½ë‹ˆë‹¤",
]
# ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ (ë¹„ì–´ ìˆìœ¼ë©´ í•„í„° ì—†ìŒ = ì „ì²´ ìˆ˜ì§‘)
ALLOWED_CATEGORIES = ["ë””ì§€í„¸ê¸°ê¸°", "ë‚¨ì„±íŒ¨ì…˜/ì¡í™”", "í‹°ì¼“/êµí™˜ê¶Œ", "eì¿ í°"]

# =========================

def _build_search_url(
    keyword: str | None = None,
    region: str | None = None,
    min_price: int | None = None,
    max_price: int | None = None,
) -> str:
    """ê²€ìƒ‰ í‚¤ì›Œë“œÂ·ì§€ì—­Â·ê°€ê²©ìœ¼ë¡œ ë‹¹ê·¼ ê²€ìƒ‰ URL ìƒì„±. ê°€ê²©ì€ price=ìµœì†Œ__ìµœëŒ€ í˜•ì‹."""
    base = "https://www.daangn.com/kr/buy-sell/"
    if keyword and keyword.strip():
        url = f"{base}?search={quote(keyword.strip())}"
    else:
        url = base
    # region ì‚¬ìš© ì‹œ: url += f"&in={quote(region)}" if "?" in url else f"?in={quote(region)}"
    if min_price is not None or max_price is not None:
        price_val = f"{min_price or ''}__{max_price or ''}"
        url += "&" if "?" in url else "?"
        url += f"price={price_val}"
    return url


def _extract_detail_js() -> str:
    """ìƒì„¸ í˜ì´ì§€ì—ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ëŠ” JS. ë‹¹ê·¼ë§ˆì¼“ DOMì— ë§ê²Œ ì…€ë ‰í„° ìˆ˜ì • ê°€ëŠ¥."""
    return """
    () => {
        const out = { title: "", description: "", image_count: "", seller_nickname: "", location: "", category: "", chat_count: "", interest_count: "", view_count: "", manner_temperature: "" };
        // ìƒì„¸ í˜ì´ì§€ ì œëª© (h1)
        const titleEl = document.querySelector('#main-content article div._4y5lbr4 h1') || document.querySelector('#main-content article h1') || document.querySelector('article h1');
        if (titleEl) out.title = titleEl.innerText.trim();
        // ê¸€ ë³¸ë¬¸ (p._4y5lbrd)
        const descEl = document.querySelector('p._4y5lbrd') || document.querySelector('#main-content article p._4y5lbrd');
        if (descEl) {
            out.description = descEl.innerText.trim().replace(/\\n+/g, " ");
        }
        if (!out.description) {
            const bodySelectors = ["article section p", "article [class*='content']", "main section p", "article p"];
            for (const sel of bodySelectors) {
                const el = document.querySelector(sel);
                if (el) {
                    out.description = el.innerText.trim().replace(/\\n+/g, " ");
                    if (out.description.length > 10) break;
                }
            }
        }
        const article = document.querySelector("article");
        if (article && !out.description) {
            const text = article.innerText.trim().replace(/\\n+/g, " ");
            if (text.length > 20) out.description = text;
        }
        // ì¹´í…Œê³ ë¦¬ (section[2] div h2 > a[href*="category_id"]: "ì—¬ì„±ì¡í™”")
        const catH2 = document.querySelector('#main-content article section:nth-of-type(2) div h2._4y5lbr9') || document.querySelector('#main-content article section:nth-of-type(2) div h2');
        if (catH2) {
            const catLink = catH2.querySelector('a[href*="category_id"]') || catH2.querySelector('a');
            if (catLink) out.category = catLink.innerText.trim();
        }
        // ì±„íŒ…/ê´€ì‹¬/ì¡°íšŒ (section[2] span._1pwsqmme: "ì±„íŒ… 22", "ê´€ì‹¬ 33", "ì¡°íšŒ 1582")
        const section2 = document.querySelector('#main-content article section:nth-of-type(2)');
        if (section2) {
            const statSpans = section2.querySelectorAll('span._1pwsqmme');
            statSpans.forEach(s => {
                const t = s.innerText.trim();
                if (t.startsWith('ì±„íŒ…')) out.chat_count = t.replace(/[^0-9]/g, '');
                else if (t.startsWith('ê´€ì‹¬')) out.interest_count = t.replace(/[^0-9]/g, '');
                else if (t.startsWith('ì¡°íšŒ')) out.view_count = t.replace(/[^0-9]/g, '');
            });
        }
        // ë§¤ë„ˆ ì˜¨ë„ (span.yzp7msi: "39.7Â°C")
        const tempSpan = document.querySelector('span.yzp7msi') || document.querySelector('#main-content article [class*="yzp7ms"] span');
        if (tempSpan && tempSpan.innerText.includes('Â°')) out.manner_temperature = tempSpan.innerText.trim();
        // íŒë§¤ì ì˜ì—­: í”„ë¡œí•„+ë‹‰ë„¤ì„+ë™ë„¤
        const profileAnchor = document.querySelector('a[aria-label*="í”„ë¡œí•„"]');
        if (profileAnchor) {
            const container = profileAnchor.closest('div');
            if (container) {
                const userLinks = container.querySelectorAll('a[href*="/kr/users/"]');
                for (const a of userLinks) {
                    const t = a.innerText.trim();
                    if (t.length > 0 && t.length < 50 && !t.includes('í”„ë¡œí•„')) {
                        out.seller_nickname = t;
                        break;
                    }
                }
                const locLink = container.querySelector('a[href*="in="]');
                if (locLink) out.location = locLink.innerText.trim();
            }
        }
        // ì´ë¯¸ì§€ ê°œìˆ˜
        const imgs = document.querySelectorAll("article img, [class*='image'] img, main img");
        if (imgs.length) out.image_count = String(imgs.length);
        return out;
    }
    """


async def _fetch_detail(page, url: str) -> dict:
    """ìƒì„¸ í˜ì´ì§€ì— ë“¤ì–´ê°€ì„œ ë³¸ë¬¸ ë“± ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œí•´ ë°˜í™˜."""
    fail_result = {"title": "", "description": "", "image_count": "", "seller_nickname": "", "location": "", "category": "", "chat_count": "", "interest_count": "", "view_count": "", "manner_temperature": ""}
    try:
        await page.goto(url, wait_until="domcontentloaded", timeout=DETAIL_PAGE_TIMEOUT_MS)
        try:
            await page.wait_for_selector(DETAIL_PAGE_WAIT_SELECTOR, timeout=DETAIL_PAGE_WAIT_TIMEOUT_MS)
        except Exception:
            await page.wait_for_timeout(DETAIL_PAGE_FALLBACK_MS)
        data = await page.evaluate(_extract_detail_js())
        return data
    except Exception as e:
        fail_result["description"] = f"[ì¡°íšŒì‹¤íŒ¨: {e!s}]"
        return fail_result


def _parse_args():
    parser = argparse.ArgumentParser(
        description="ë‹¹ê·¼ë§ˆì¼“ ê²€ìƒ‰ í¬ë¡¤ë§",
        epilog="ì˜ˆì‹œ:  python carrot-rough-crawl.py --keyword ì•„ì´í° --categories ë””ì§€í„¸ê¸°ê¸°,í‹°ì¼“/êµí™˜ê¶Œ",
    )
    parser.add_argument("--keyword", "-k", default=None, help="ê²€ìƒ‰ í‚¤ì›Œë“œ (ìƒëµ ì‹œ ì „ì²´ ë¦¬ìŠ¤íŠ¸)")
    parser.add_argument(
        "--categories", "-c",
        default=None,
        help="ìˆ˜ì§‘í•  ì¹´í…Œê³ ë¦¬ (ì‰¼í‘œ êµ¬ë¶„). ì˜ˆ: ë””ì§€í„¸ê¸°ê¸°,ë‚¨ì„±íŒ¨ì…˜/ì¡í™”,í‹°ì¼“/êµí™˜ê¶Œ,eì¿ í°. ë¹„ìš°ë©´ ìŠ¤í¬ë¦½íŠ¸ ê¸°ë³¸ê°’ ì‚¬ìš©, --no-filter ì´ë©´ ì „ì²´ ìˆ˜ì§‘",
    )
    parser.add_argument("--no-filter", action="store_true", help="ì¹´í…Œê³ ë¦¬ í•„í„° ì—†ì´ ì „ì²´ ìˆ˜ì§‘")
    parser.add_argument("--min-price", type=int, default=None, metavar="N", help="ê°€ê²© ìµœì†Œê°’ (ì›). ì˜ˆ: 50000")
    parser.add_argument("--max-price", type=int, default=None, metavar="N", help="ê°€ê²© ìµœëŒ€ê°’ (ì›). ì˜ˆ: 10000000")
    # parser.add_argument("--region", "-r", help="ë™ë„¤ (ë™ì´ë¦„-ì½”ë“œ, ì˜ˆ: ì—­ì‚¼ë™-6035). ë¯¸ì‚¬ìš© ì‹œ ë‚´ ìœ„ì¹˜ ê¸°ì¤€")
    return parser.parse_args()


async def main(
    keyword: str | None = None,
    allowed_categories: list[str] | None = None,
    no_filter: bool = False,
    min_price: int | None = None,
    max_price: int | None = None,
):
    if no_filter:
        allowed_set = None
    elif allowed_categories is None:
        allowed_set = set(ALLOWED_CATEGORIES)
    else:
        allowed_set = set(allowed_categories) if allowed_categories else None

    search_url = _build_search_url(keyword, min_price=min_price, max_price=max_price)
    print("ê²€ìƒ‰ URL:", search_url)
    print("í‚¤ì›Œë“œ:", keyword if (keyword and keyword.strip()) else "(ì—†ìŒ)")
    if min_price is not None or max_price is not None:
        print("ê°€ê²© ì¡°ê±´:", f"{min_price or '?'}ì› ~ {max_price or '?'}ì›")
    if allowed_set:
        print("ì¹´í…Œê³ ë¦¬ í•„í„°:", ", ".join(sorted(allowed_set)))
    else:
        print("ì¹´í…Œê³ ë¦¬ í•„í„°: ì—†ìŒ (ì „ì²´ ìˆ˜ì§‘)")

    start_time = time.perf_counter()
    print("í¬ë¡¤ë§ ì‹œì‘")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=HEADLESS,
            slow_mo=SLOW_MO
        )
        page = await browser.new_page()

        await page.goto(search_url, wait_until="domcontentloaded")
        await page.wait_for_selector(ITEM_SELECTOR, timeout=LIST_PAGE_WAIT_SELECTOR_TIMEOUT_MS)

        print("í˜ì´ì§€ íƒ€ì´í‹€:", await page.title())

        prev_count = 0

        # =========================
        # ğŸ”¥ ë”ë³´ê¸° ë°˜ë³µ
        # =========================
        while True:
            cards = page.locator(ITEM_SELECTOR)
            count = await cards.count()
            print("í˜„ì¬ ê°œìˆ˜:", count)

            if count >= TARGET_COUNT:
                print("ëª©í‘œ ê°œìˆ˜ ë„ë‹¬")
                break

            if count == prev_count:
                print("ë” ì´ìƒ ì¦ê°€í•˜ì§€ ì•ŠìŒ")
                break

            prev_count = count

            more_btn = page.locator(MORE_BUTTON_SELECTOR)

            if await more_btn.count() == 0:
                print("ë”ë³´ê¸° ë²„íŠ¼ ì—†ìŒ â†’ ì¢…ë£Œ")
                break

            if not await more_btn.is_enabled():
                print("ë”ë³´ê¸° ë²„íŠ¼ ë¹„í™œì„±í™” â†’ ì¢…ë£Œ")
                break

            try:
                await more_btn.click()
                deadline = time.monotonic() + MORE_BUTTON_POLL_MAX_MS / 1000
                while True:
                    await asyncio.sleep(MORE_BUTTON_POLL_INTERVAL_MS / 1000)
                    new_count = await cards.count()
                    if new_count > prev_count:
                        break
                    if time.monotonic() >= deadline:
                        break
            except Exception as e:
                print("ë”ë³´ê¸° í´ë¦­ ì‹¤íŒ¨:", e)
                break

        # =========================
        # ë°ì´í„° ì¶”ì¶œ
        # =========================
        
        items = await page.evaluate("""
        () => {
            const cards = document.querySelectorAll("a[data-gtm='search_article']");
            const results = [];

            cards.forEach(card => {

                const href = card.getAttribute("href") || "";
                const fullUrl = href ? "https://www.daangn.com" + href : "";

                // -------------------------
                // 1ï¸âƒ£ wrapper
                // -------------------------
                const wrapper = card.querySelector(":scope > div");
                if (!wrapper) return;

                // wrapper ì•ˆì—
                // [0] ì¸ë„¤ì¼ ì˜ì—­
                // [1] í…ìŠ¤íŠ¸ ì˜ì—­
                const children = wrapper.querySelectorAll(":scope > div");
                if (children.length < 2) return;

                const thumbnailArea = children[0];
                const textContainer = children[1];

                // -------------------------
                // 2ï¸âƒ£ íŒë§¤ìƒíƒœ (ì¸ë„¤ì¼ ì˜ì—­ ì•ˆ)
                // -------------------------
                let status = "íŒë§¤ì¤‘";
                const statusSpan = thumbnailArea.querySelector("span");
                if (statusSpan) {
                    const text = statusSpan.innerText.trim();
                    if (text === "ì˜ˆì•½ì¤‘" || text === "ê±°ë˜ì™„ë£Œ") {
                        status = text;
                    }
                }

                // -------------------------
                // 3ï¸âƒ£ info / meta ë¶„ë¦¬
                // -------------------------
                const textDivs = textContainer.querySelectorAll(":scope > div");
                if (textDivs.length < 2) return;

                const infoDiv = textDivs[0];
                const metaDiv = textDivs[1];

                const spans = infoDiv.querySelectorAll("span");

                const title = spans[0]?.innerText?.trim() || "";
                const price = spans[1]?.innerText?.trim() || "";

                const location = metaDiv.querySelector("span span")?.innerText?.trim() || "";
                const time = metaDiv.querySelector("time")?.innerText?.trim() || "";
                const categoryEl = card.querySelector('a[href*="category_id"]');
                const category = categoryEl ? categoryEl.innerText.trim() : "";

                if (!title) return;

                results.push({
                    title,
                    price,
                    location,
                    time,
                    status,
                    url: fullUrl,
                    category,
                    description: "",
                    image_count: ""
                });
            });

            return results;
        }
        """)

        # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ëŒì˜¬ ë“±ìœ¼ë¡œ ê°™ì€ ê¸€ì´ ì—¬ëŸ¬ ë²ˆ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ)
        seen_urls = set()
        items_deduped = []
        for i in items:
            u = i.get("url") or ""
            if u and u not in seen_urls:
                seen_urls.add(u)
                items_deduped.append(i)
        if len(items_deduped) < len(items):
            print(f"URL ì¤‘ë³µ ì œê±°: {len(items)} â†’ {len(items_deduped)}ê±´")
        items = items_deduped

        print(f"ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘ ê°œìˆ˜: {len(items)}")

        # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì¹´í…Œê³ ë¦¬ ì•Œ ìˆ˜ ìˆìœ¼ë©´ ë¯¸ë¦¬ í•„í„° â†’ ìƒì„¸ ë°©ë¬¸ íšŸìˆ˜ ê°ì†Œ
        if allowed_set:
            known_allowed = [i for i in items if i.get("category") in allowed_set]
            unknown = [i for i in items if not (i.get("category") or "").strip()]
            items_to_detail = known_allowed + unknown  # í—ˆìš©ëœ ê²ƒ + ì¹´í…Œê³ ë¦¬ ë¯¸í™•ì¸(ìƒì„¸ì—ì„œ í™•ì¸)
            skipped = len(items) - len(items_to_detail)
            if skipped > 0:
                print(f"ì¹´í…Œê³ ë¦¬ í•„í„°ë¡œ ìƒì„¸ ìƒëµ: {skipped}ê±´ (ìƒì„¸ ìˆ˜ì§‘ ëŒ€ìƒ: {len(items_to_detail)}ê±´)")
        else:
            items_to_detail = items

        # =========================
        # ğŸ”¥ ìƒì„¸ í˜ì´ì§€ ì¶”ê°€ ìˆ˜ì§‘ (ë³‘ë ¬)
        # =========================
        total = len(items_to_detail)
        concurrency = min(DETAIL_PAGE_CONCURRENCY, total) if total else 0
        detail_pages = []

        if total > 0 and concurrency > 0:
            detail_pages = [await browser.new_page() for _ in range(concurrency)]
            print(f"ìƒì„¸ ìˆ˜ì§‘ ë³‘ë ¬ ìˆ˜: {concurrency}")

        for chunk_start in range(0, total, concurrency if concurrency else 1):
            chunk = items_to_detail[chunk_start : chunk_start + concurrency]
            for i, item in enumerate(chunk):
                print(f"ìƒì„¸ í˜ì´ì§€ ìˆ˜ì§‘ ì¤‘ {chunk_start + i + 1}/{total} - {item.get('title', '')[:30]}...")
            tasks = [_fetch_detail(detail_pages[j], chunk[j]["url"]) for j in range(len(chunk))]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            for j, item in enumerate(chunk):
                r = results[j]
                if isinstance(r, Exception):
                    extra = {"title": "", "description": f"[ì¡°íšŒì‹¤íŒ¨: {r!s}]", "image_count": "", "seller_nickname": "", "location": "", "category": "", "chat_count": "", "interest_count": "", "view_count": "", "manner_temperature": ""}
                else:
                    extra = r
                if extra.get("title"):
                    item["title"] = extra["title"]
                item["description"] = extra.get("description", "")
                item["image_count"] = extra.get("image_count", "")
                item["seller_nickname"] = extra.get("seller_nickname", "")
                if extra.get("location"):
                    item["location"] = extra["location"]
                item["category"] = extra.get("category", "")
                item["chat_count"] = extra.get("chat_count", "")
                item["interest_count"] = extra.get("interest_count", "")
                item["view_count"] = extra.get("view_count", "")
                item["manner_temperature"] = extra.get("manner_temperature", "")
            any_fail = any(
                isinstance(r, Exception) or ((r.get("description") or "").startswith("[ì¡°íšŒì‹¤íŒ¨") if isinstance(r, dict) else False)
                for r in results
            )
            delay_ms = DETAIL_PAGE_DELAY_MS_ON_FAIL if any_fail else DETAIL_PAGE_DELAY_MS
            if chunk_start + len(chunk) < total:
                await asyncio.sleep(delay_ms / 1000)

        for p in detail_pages:
            await p.close()

        # ìƒì„¸ì—ì„œ í™•ì¸í•œ ì¹´í…Œê³ ë¦¬ë¡œ í•œ ë²ˆ ë” í•„í„° (ì¹´í…Œê³ ë¦¬ ë¯¸í™•ì¸ã ã£ãŸê±´ í¬í•¨)
        if allowed_set:
            items_to_write = [i for i in items_to_detail if i.get("category") in allowed_set]
            print(f"ì¹´í…Œê³ ë¦¬ í•„í„° ê²°ê³¼: {len(items_to_write)}ê±´ ì €ì¥")
        else:
            items_to_write = items_to_detail

        # =========================
        # ğŸ”¥ CSV ì €ì¥ (results/ë…„-ì›”-ì¼-ì‹œ-ë¶„-ì´ˆ.csv)
        # =========================
        RESULTS_DIR.mkdir(parents=True, exist_ok=True)
        timestamp = datetime.now().strftime("%Y-%m-%d-%H%M%S")
        out_path = RESULTS_DIR / f"{timestamp}.csv"
        fieldnames = ["title", "price", "location", "time", "status", "category", "seller_nickname", "image_count", "chat_count", "interest_count", "view_count", "manner_temperature", "description", "url"]
        with open(out_path, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            writer.writeheader()
            writer.writerows(items_to_write)

        print(f"{out_path} ì €ì¥ ì™„ë£Œ")

        elapsed = time.perf_counter() - start_time
        m, s = divmod(int(elapsed), 60)
        if m > 0:
            print(f"ì´ í¬ë¡¤ë§ ì‹œê°„: {m}ë¶„ {s}ì´ˆ ({elapsed:.1f}ì´ˆ)")
        else:
            print(f"ì´ í¬ë¡¤ë§ ì‹œê°„: {elapsed:.1f}ì´ˆ")

        await browser.close()


if __name__ == "__main__":
    args = _parse_args()
    if args.categories is not None:
        allowed = [c.strip() for c in args.categories.split(",") if c.strip()]
    else:
        allowed = None
    asyncio.run(main(
        keyword=args.keyword,
        allowed_categories=allowed,
        no_filter=args.no_filter,
        min_price=args.min_price,
        max_price=args.max_price,
    ))
